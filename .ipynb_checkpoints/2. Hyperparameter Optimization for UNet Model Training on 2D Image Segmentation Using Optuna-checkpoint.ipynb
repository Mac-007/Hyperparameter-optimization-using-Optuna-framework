{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca48d23-c43f-4c70-8026-9876c4d6d5e9",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of optimizing hyperparameters for training a UNet model on a 2D image segmentation task using Optuna. <br> It covers data preparation, model definition, and the use of Optuna to find the best hyperparameters by minimizing validation loss. Additionally, it includes code for retraining the model with the optimal parameters and saving the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135e6e8-7658-4b5a-85c5-d83c5fa11406",
   "metadata": {},
   "source": [
    "<b>Dataset Link:- </b> https://www.kaggle.com/datasets/kmader/finding-lungs-in-ct-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14cf2e-7bc1-4f3a-b055-9628d87bdbfa",
   "metadata": {},
   "source": [
    "<h4> Import Required Libraries </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2eaf8-00e6-47b0-92ab-273d97238aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f05f8-9393-408f-be50-1b0c8d93b870",
   "metadata": {},
   "source": [
    "<h4>Root dir path initialization </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924183f-9a5f-473d-b31b-bf9854684f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "root_dir = \"/home/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b42d3-ba21-4b0b-8335-4b81a45feee3",
   "metadata": {},
   "source": [
    "<h4> Collect all the image and mask files along with it's path </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b576ee5-7eae-44b6-80de-7d7e5893e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and mask file paths\n",
    "image_files = sorted(glob.glob(root_dir + '2d_images/' + '*.tif'))\n",
    "mask_files = sorted(glob.glob(root_dir + '2d_masks/' + '*.tif'))\n",
    "\n",
    "# Check if the number of images matches the number of masks\n",
    "if len(image_files) != len(mask_files):\n",
    "    raise ValueError(\"Number of image files and mask files do not match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e42866-758f-4e6c-aa2a-303c787dc651",
   "metadata": {},
   "source": [
    "<h4> Dataset split for train, val and test </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc02daa-5a2b-45f8-bdc0-dca0ca755bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_image_files, test_image_files, train_mask_files, test_mask_files = train_test_split(\n",
    "    image_files, mask_files, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_image_files, val_image_files, train_mask_files, val_mask_files = train_test_split(\n",
    "    train_image_files, train_mask_files, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Number of training image files: {len(train_image_files)}\")\n",
    "print(f\"Number of validation image files: {len(val_image_files)}\")\n",
    "print(f\"Number of testing image files: {len(test_image_files)}\")\n",
    "print(f\"Total number of image files: {len(train_image_files) + len(val_image_files) + len(test_image_files)}\")\n",
    "print(f\"Total number of mask files: {len(train_mask_files) + len(val_mask_files) + len(test_mask_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e40a8c-9ec8-4874-81f3-993cc9180f2f",
   "metadata": {},
   "source": [
    "<h4> Check Pixel value range </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea687ff5-b42c-458b-8f37-0073ef4b6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pixel value range\n",
    "temp_image = Image.open(mask_files[0])\n",
    "temp_image_array = np.array(temp_image)\n",
    "print(\"Image shape:\", temp_image_array.shape)\n",
    "print(f\"Pixel value range: {temp_image_array.min()} to {temp_image_array.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809324a6-c4e7-43d6-a921-6338c1b56ea4",
   "metadata": {},
   "source": [
    "<h4> Convert the mask values into [0,1] </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebce3d-4a3d-4ce5-8897-eda3be81a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_0_and_1(mask_image):\n",
    "    mask_image_array = np.array(mask_image)\n",
    "    binary_mask_array = np.where(mask_image_array > 0, 1, 0)\n",
    "    return binary_mask_array.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e165f-5182-4c34-9e15-9e7964c70a78",
   "metadata": {},
   "source": [
    "<h4> Dataloader for Custom dataset </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd08a0-ce3e-49fc-8248-d307ee2a9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        mask_path = self.mask_files[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        mask = convert_to_0_and_1(mask)\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        mask = mask.squeeze(0).unsqueeze(0).float()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0df524-6601-43f4-a7e8-cd148e10d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(image_files=train_image_files, mask_files=train_mask_files, transform=transform)\n",
    "val_dataset = CustomDataset(image_files=val_image_files, mask_files=val_mask_files, transform=transform)\n",
    "test_dataset = CustomDataset(image_files=test_image_files, mask_files=test_mask_files, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab66b15-7974-490b-a377-80e99a44bb72",
   "metadata": {},
   "source": [
    "<h4> UNet Model architecture </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbcc31-00d7-44b5-83f9-c116888c98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UNet model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = self.conv_block(1, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.enc5 = self.conv_block(512, 1024)\n",
    "        self.upconv5 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = self.conv_block(1024, 512)\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(128, 64)\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n",
    "        enc5 = self.enc5(F.max_pool2d(enc4, 2))\n",
    "        dec4 = self.upconv5(enc5)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.dec4(dec4)\n",
    "        dec3 = self.upconv4(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "        dec2 = self.upconv3(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        dec1 = self.upconv2(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "        return torch.sigmoid(self.final_conv(dec1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c2d09-f9a9-450f-81d2-bb767e032b39",
   "metadata": {},
   "source": [
    "<h4>objective function for Optuna</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8147e-53ef-4146-b1b9-1523178eabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 20, 500)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32])\n",
    "    \n",
    "    # Define the device to be used for training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Define the model, criterion, and optimizer\n",
    "    model = UNet().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create data loaders with the suggested batch size\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_dataloader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        epoch_train_loss /= len(train_dataloader)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_dataloader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            # Optionally save the model here\n",
    "\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de120d7-5929-4e4f-a217-39ea7a6b27e0",
   "metadata": {},
   "source": [
    "<h4>Initialize Optuna study and optimize</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b48c72-beb5-4aa4-b2f6-548fbafbce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print and save the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "print(\"Best Value (Validation Loss):\")\n",
    "print(study.best_value)\n",
    "\n",
    "# Save the study results\n",
    "with open('optuna_study_results.json', 'w') as f:\n",
    "    json.dump(study.best_params, f)\n",
    "    json.dump(study.best_value, f)\n",
    "\n",
    "# Initialize the final model with the best hyperparameters and retrain if desired\n",
    "best_params = study.best_params\n",
    "best_model = UNet().to(device)\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "best_criterion = nn.BCELoss()\n",
    "\n",
    "# Retrain with best parameters\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_dataloader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = best_model(images)\n",
    "        loss = best_criterion(outputs, masks)\n",
    "        \n",
    "        best_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = best_model(images)\n",
    "            loss = best_criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{best_params[\"num_epochs\"]}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "torch.save(best_model.state_dict(), 'best_model_with_optuna.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
