{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906760eb-2536-4d98-aea9-efa0140c02e2",
   "metadata": {},
   "source": [
    "<h1>Autograd: Automatic Differentiation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ab511-4b09-46ef-b4ea-b16adb7826b2",
   "metadata": {},
   "source": [
    "References:- https://youtu.be/VMj-3S1tku0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55599726-0a31-4bdf-ba5c-d40c53e7fd8e",
   "metadata": {},
   "source": [
    "PyTorch’s autograd package provides automatic differentiation for all operations on Tensors. <br>\n",
    "This is especially useful for training neural networks. autograd tracks all operations on the tensor, allowing you to automatically compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db3ebe-392b-4566-8ee5-f715beef1383",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Creating Tensors with requires_grad:</b> Enable gradient computation.</li>\n",
    "<li><b>Backward Propagation:</b> Compute gradients.</li>\n",
    "<li><b>Gradient Example:</b> Simple gradient computation.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d597315-2430-4764-8d79-269a8e748e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor with requires_grad:\n",
      " tensor([1., 2., 3.], requires_grad=True)\n",
      "\n",
      "Result Tensor after Operations:\n",
      " tensor([ 7.,  9., 11.], grad_fn=<AddBackward0>)\n",
      "\n",
      "Gradient of the Tensor:\n",
      " tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# Create Tensor with Gradient Tracking\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(\"\\nTensor with requires_grad:\\n\", tensor)\n",
    "\n",
    "# Perform Operations\n",
    "result = tensor * 2 + 5\n",
    "print(\"\\nResult Tensor after Operations:\\n\", result)\n",
    "\n",
    "# Compute Gradients\n",
    "result_sum = result.sum()\n",
    "result_sum.backward()\n",
    "print(\"\\nGradient of the Tensor:\\n\", tensor.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb4856-c26f-4ee6-9edd-2e0638138d2c",
   "metadata": {},
   "source": [
    "Autograd is a tool that records operations on tensors to automatically compute gradients during backpropagation.<br> \n",
    "It uses a dynamic computational graph, which is constructed on-the-fly during the forward pass and used to compute gradients during the backward pass.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b8f35-a355-451e-8e66-d5e0dc0770db",
   "metadata": {},
   "source": [
    "<h4>Core Components of Autograd:</h4>\n",
    "<ul>\n",
    "<li><b>Tensor with requires_grad:</b> Tensors with this attribute track operations for gradient computation.</li>\n",
    "<li><b>Computational Graph:</b> Dynamic graph constructed from tensor operations to compute gradients.</li>\n",
    "<li><b>Backward Propagation:</b> Computes the gradient of a tensor with respect to some loss function.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad9194-151c-4f42-8f20-dec492f2e96e",
   "metadata": {},
   "source": [
    "<h4>Basic Operations with Autograd</h4>\n",
    "<ol>\n",
    "<li><b>Creating Tensors with Gradient Tracking:</b></li>\n",
    "To enable gradient computation, set <b>requires_grad=True</b> when creating a tensor.\n",
    "\n",
    "<li><b>Performing Operations:</b></li>\n",
    "Perform operations on tensors. The computational graph records these operations.\n",
    "\n",
    "<li><b>Computing Gradients:</b></li>\n",
    "Call .backward() on a tensor to compute gradients.\n",
    "\n",
    "<li><b>Accessing Gradients:</b></li>\n",
    "Access the gradients through the .grad attribute of a tensor.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1eb82876-e224-45c1-af33-945c2760a662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x:\n",
      " tensor([2., 3.], requires_grad=True)\n",
      "Tensor y:\n",
      " tensor([4., 5.], requires_grad=True)\n",
      "Tensor z (x * y):\n",
      " tensor([ 8., 15.], grad_fn=<MulBackward0>)\n",
      "Sum of z (out):\n",
      " tensor(23., grad_fn=<SumBackward0>)\n",
      "Gradient of x:\n",
      " tensor([4., 5.])\n",
      "Gradient of y:\n",
      " tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Create Tensors with requires_grad=True\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Step 2: Perform Operations\n",
    "z = x * y  # Element-wise multiplication\n",
    "out = z.sum()  # Sum of all elements\n",
    "\n",
    "print(\"Tensor x:\\n\", x)\n",
    "print(\"Tensor y:\\n\", y)\n",
    "print(\"Tensor z (x * y):\\n\", z)\n",
    "print(\"Sum of z (out):\\n\", out)\n",
    "\n",
    "# Step 3: Compute Gradients\n",
    "out.backward()  # Compute gradients\n",
    "\n",
    "# Access gradients\n",
    "print(\"Gradient of x:\\n\", x.grad)  # Gradient of out with respect to x\n",
    "print(\"Gradient of y:\\n\", y.grad)  # Gradient of out with respect to y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8bfad-6b22-48ac-90f7-bd0db5d21f4f",
   "metadata": {},
   "source": [
    "<h4>Another Example</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ca0cb4-b659-4926-b021-6b4f8c79f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0104]])\n",
      "tensor([-3.0104])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple linear model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Generate dummy data\n",
    "inputs = torch.tensor([[1.0]], requires_grad=True)\n",
    "target = torch.tensor([[2.0]])\n",
    "\n",
    "# Forward pass: Compute predicted y by passing x to the model\n",
    "predicted = model(inputs)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(predicted, target)\n",
    "\n",
    "# Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(model.weight.grad)  # Print gradients of weights\n",
    "print(model.bias.grad)    # Print gradients of biases\n",
    "\n",
    "# Update weights using optimizer\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90909b-9a9c-46ae-8068-1b3e75578836",
   "metadata": {},
   "source": [
    "<h4>Practical Use Case: Training a Neural Network</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e0e94-2679-41b7-bd58-b8a8a6a05bd8",
   "metadata": {},
   "source": [
    "In training a neural network, <b>autograd</b> is used to compute gradients for backpropagation.<br>\n",
    "Here’s a simplified example of how autograd is used during the training process:\n",
    "steps:\n",
    "<ol>\n",
    "    <li>Model Architecture define</li>\n",
    "    <li>Forward pass</li>\n",
    "    <li>Loss calculation </li>\n",
    "    <li>Optimization</li>\n",
    "    <li>Backward pass</li>\n",
    "    <li>Update weights</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "852be4be-4dfd-4560-8bf3-570cca78b575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "Epoch 1, Loss: 40.10003662109375\n",
      "Epoch 2, Loss: 31.702224731445312\n",
      "Epoch 3, Loss: 25.064176559448242\n",
      "Epoch 4, Loss: 19.817129135131836\n",
      "Epoch 5, Loss: 15.669593811035156\n",
      "Epoch 6, Loss: 12.391161918640137\n",
      "Epoch 7, Loss: 9.799714088439941\n",
      "Epoch 8, Loss: 7.751287460327148\n",
      "Epoch 9, Loss: 6.132090091705322\n",
      "Epoch 10, Loss: 4.852177619934082\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1)  # One linear layer | (batch_size, 1)\n",
    "        '''\n",
    "            self.fc = nn.Linear(in_features, out_features)\n",
    "            in_features =>  number of input features (dimensions).\n",
    "            out_features => number of output features (dimensions).\n",
    "\n",
    "        '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the network, loss function, and optimizer\n",
    "net = SimpleNet()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "# Sample data\n",
    "inputs = torch.tensor([[1.0], [2.0], [3.0]]) # (3,1) | Your input tensor inputs has a shape of (3, 1). This means you have 3 samples, and each sample has 1 features\n",
    "targets = torch.tensor([[2.0], [4.0], [6.0]]) # (3,1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebda824-5e68-4f38-819d-1cb08f7bc18c",
   "metadata": {},
   "source": [
    "<b>Explanation:<b>\n",
    "<ul>\n",
    "<li><b>Network Definition:</b> A simple neural network with one linear layer.</li>\n",
    "<li>Training Loop:</li>\n",
    "    <ul>\n",
    "        <li> <b>Forward Pass:</b> Compute network outputs.</li>\n",
    "        <li> <b>Compute Loss:</b> Measure the error between outputs and targets.</li>\n",
    "        <li> <b>Backward Pass:</b> Compute gradients using .backward().</li>\n",
    "        <li> <b>Update Weights:</b> Adjust the model’s parameters using the optimizer.</li>\n",
    "</ul>\n",
    "Autograd simplifies the process of computing gradients, making it a core component in building and training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7eafd2-4362-4715-ac3b-8fbc6bb43371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d937e7-db1c-42ec-96c8-d272f7af4cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
