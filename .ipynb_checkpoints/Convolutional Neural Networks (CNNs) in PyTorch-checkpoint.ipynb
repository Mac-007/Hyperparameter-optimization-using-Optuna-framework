{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c8750d-6796-4e2d-a2b9-78830fd479db",
   "metadata": {},
   "source": [
    "<h1>Convolutional Neural Networks (CNNs) in PyTorch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395dc5e-2e04-4d03-b190-92c840cf6c77",
   "metadata": {},
   "source": [
    "Let's build a simple CNN for image classification using the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e94b97-0747-4337-a0ab-60ebb791b038",
   "metadata": {},
   "source": [
    "<h5><b>Step 1:</b> Import Libraries and Prepare Dataset</h5>\n",
    "First, import the necessary libraries and load the CIFAR-10 dataset using <b>torchvision</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bfb5dd-f9ce-455a-bd90-34df784c6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transformations for the training set\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomCrop(32, padding=4),  #  Before performing the random crop it applies padding of 4 pixels,  Randomly crop the image | width and height of the random crop will be 32 pixels\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    # Normalize the image | first () - mean values for each of the three color\n",
    "    # channels (Red, Green, and Blue respectively) of the input images and second () -  standard deviation values for each of the three color channels \n",
    "    # (Red, Green, and Blue respectively) of the input images.\n",
    "'''\n",
    "        Normalization is used to adjust the pixel values of the input image so that they have a mean of 0 and a standard deviation of 1. \n",
    "        This typically leads to faster convergence during training and can improve the overall performance of the neural network.\n",
    "'''\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # 10 Classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417df03d-8f06-424a-a3c3-f33b63811852",
   "metadata": {},
   "source": [
    "<h5><b>Step 2:</b> Define the CNN Architecture</h5>\n",
    "Next, define the CNN architecture by creating a class that inherits from <b>nn.Module.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b93b5a-effb-4dc7-956a-c133603fc00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # synatx -> nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # 3 input channels, 16 output channels or feature maps, 3x3 kernel (16 output channel is going to match with next layer's input which is 16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1) # does not change the spatial dimensions (due to padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)               # 2x2 max pooling | This layer reduces the spatial dimensions of the feature maps by a factor of 2, effectively performing downsampling.\n",
    "        # syntax - > nn.Linear(in_features, out_features)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 120)   # Fully Connected Layer | 120 Output features\n",
    "        \n",
    "        # Before feeding the data into the fully connected layer, the output feature map needs to be flattened. \n",
    "        # Because, Fully connected layers operate on 1D vectors. Each neuron in a fully connected layer has connections to every element of the input vector. \n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)           # Fully Connected Layer | 120 input features, 84 Ouput features\n",
    "        self.fc3 = nn.Linear(84, 10)            # Fully Connected Layer | 84 input features, 10 output features (As we have 10 Classes/labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # ReLU activation function | self.pool is an instance of a pooling layer defined earlier in the model, \n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # ReLU activation function | self.pool is an instance of a pooling layer defined earlier in the model, \n",
    "        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor\n",
    "                                    # The view method in PyTorch is used to reshape a tensor without changing its data.\n",
    "                                    # the -1 dimension based on the total number of elements in the tensor and the sizes of the other specified dimensions.\n",
    "                                    # The -1 tells PyTorch to infer the size of the first dimension (batch_size) so that the total number of elements remains consistent.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "net = SimpleCNN()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc518bd5-09f6-402d-9fa5-fe2b7f71845d",
   "metadata": {},
   "source": [
    "<b>(Optional) To print the detailed above architecture</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7a2e37-b940-433e-8b31-d3a6aeab4c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "         MaxPool2d-2           [-1, 16, 16, 16]               0\n",
      "            Conv2d-3           [-1, 32, 16, 16]           4,640\n",
      "         MaxPool2d-4             [-1, 32, 8, 8]               0\n",
      "            Linear-5                  [-1, 120]         245,880\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 261,982\n",
      "Trainable params: 261,982\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 1.00\n",
      "Estimated Total Size (MB): 1.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(net,(3,32,32)) # Example input size for a model expecting 32x32 images with 3 channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105a3f5-c65a-4c6a-9b3f-2f0e17c61b86",
   "metadata": {},
   "source": [
    "<b>(Optional) Or, to show the defined model architecture in form of diagram using torchviz</b>\n",
    "\n",
    "\"torchviz\" expecting Graphviz (https://graphviz.org/download/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d12cd286-bd90-4ac0-992c-4339f9359e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Graphviz\\bin\\dot.exe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will check and fix Graphviz Path issue, if anythime exist \n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Adjust the path to where Graphviz is installed\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\Program Files\\Graphviz\\bin'\n",
    "\n",
    "\n",
    "# Check if the dot executable is available in the PATH\n",
    "dot_path = subprocess.run([\"where\", \"dot\"], capture_output=True, text=True).stdout\n",
    "print(dot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e3f7732-4591-462a-8cc5-20db619c88f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_architecture.png'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32) # Returns a tensor with random numbers from the standard normal distribution.\n",
    "                              # Batch size of 1, 3 color channels, 32x32 pixels\n",
    "y = net(x) \n",
    "\n",
    "make_dot(y, params=dict(net.named_parameters())).render(\"model_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d123599-3010-4bc9-be64-71ee4eed32e4",
   "metadata": {},
   "source": [
    "<b>(Optional) Generate architecture diamgra, Using TensorBoard </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "090521a5-5dd1-49e6-9d58-b4c6af824fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6a85b86-697e-485e-98c7-95c0b972f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SummaryWriter to write to TensorBoard\n",
    "writer = SummaryWriter('runs/simple_model')\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32) \n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "writer.add_graph(net, x)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "851dc2ea-28de-4931-ad94-ee4ef5b02fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs    # While this is runing, visit to http://localhost:6006, to check the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3123750-9e14-4323-97b9-eaa33066189a",
   "metadata": {},
   "source": [
    "<h5><b>Step 3:</b> Define Loss Function and Optimizer</h5>\n",
    "\n",
    "<b>CrossEntropyLoss:</b> Combines nn.LogSoftmax() and nn.NLLLoss() in one single class.<br>\n",
    "<b>SGD Optimizer:</b> Updates the network parameters using stochastic gradient descent with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fa35b49-8d02-4a65-83f8-3ab3c39a0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # Stochastic Gradient Descent with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12e233-45ad-4270-8696-f6ca085877e4",
   "metadata": {},
   "source": [
    "<h5><b>Step 4:</b> Train the CNN</h5>\n",
    "Implement the training loop to train the CNN on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "731fc9e5-cdc9-4eb7-8f9d-db64cb86489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/500], Loss: 2.3029\n",
      "Epoch [1/10], Step [200/500], Loss: 2.2929\n",
      "Epoch [1/10], Step [300/500], Loss: 2.2777\n",
      "Epoch [1/10], Step [400/500], Loss: 2.2400\n",
      "Epoch [1/10], Step [500/500], Loss: 2.1573\n",
      "Epoch [2/10], Step [100/500], Loss: 2.0751\n",
      "Epoch [2/10], Step [200/500], Loss: 2.0145\n",
      "Epoch [2/10], Step [300/500], Loss: 1.9694\n",
      "Epoch [2/10], Step [400/500], Loss: 1.9443\n",
      "Epoch [2/10], Step [500/500], Loss: 1.9008\n",
      "Epoch [3/10], Step [100/500], Loss: 1.8522\n",
      "Epoch [3/10], Step [200/500], Loss: 1.8084\n",
      "Epoch [3/10], Step [300/500], Loss: 1.7791\n",
      "Epoch [3/10], Step [400/500], Loss: 1.7508\n",
      "Epoch [3/10], Step [500/500], Loss: 1.6969\n",
      "Epoch [4/10], Step [100/500], Loss: 1.6780\n",
      "Epoch [4/10], Step [200/500], Loss: 1.6539\n",
      "Epoch [4/10], Step [300/500], Loss: 1.6397\n",
      "Epoch [4/10], Step [400/500], Loss: 1.6091\n",
      "Epoch [4/10], Step [500/500], Loss: 1.6146\n",
      "Epoch [5/10], Step [100/500], Loss: 1.5933\n",
      "Epoch [5/10], Step [200/500], Loss: 1.5711\n",
      "Epoch [5/10], Step [300/500], Loss: 1.5560\n",
      "Epoch [5/10], Step [400/500], Loss: 1.5289\n",
      "Epoch [5/10], Step [500/500], Loss: 1.5308\n",
      "Epoch [6/10], Step [100/500], Loss: 1.5451\n",
      "Epoch [6/10], Step [200/500], Loss: 1.5188\n",
      "Epoch [6/10], Step [300/500], Loss: 1.4934\n",
      "Epoch [6/10], Step [400/500], Loss: 1.4825\n",
      "Epoch [6/10], Step [500/500], Loss: 1.4714\n",
      "Epoch [7/10], Step [100/500], Loss: 1.4594\n",
      "Epoch [7/10], Step [200/500], Loss: 1.4561\n",
      "Epoch [7/10], Step [300/500], Loss: 1.4420\n",
      "Epoch [7/10], Step [400/500], Loss: 1.4455\n",
      "Epoch [7/10], Step [500/500], Loss: 1.4366\n",
      "Epoch [8/10], Step [100/500], Loss: 1.4194\n",
      "Epoch [8/10], Step [200/500], Loss: 1.4053\n",
      "Epoch [8/10], Step [300/500], Loss: 1.4108\n",
      "Epoch [8/10], Step [400/500], Loss: 1.4094\n",
      "Epoch [8/10], Step [500/500], Loss: 1.3951\n",
      "Epoch [9/10], Step [100/500], Loss: 1.3747\n",
      "Epoch [9/10], Step [200/500], Loss: 1.3932\n",
      "Epoch [9/10], Step [300/500], Loss: 1.3732\n",
      "Epoch [9/10], Step [400/500], Loss: 1.3661\n",
      "Epoch [9/10], Step [500/500], Loss: 1.3572\n",
      "Epoch [10/10], Step [100/500], Loss: 1.3456\n",
      "Epoch [10/10], Step [200/500], Loss: 1.3397\n",
      "Epoch [10/10], Step [300/500], Loss: 1.3553\n",
      "Epoch [10/10], Step [400/500], Loss: 1.3272\n",
      "Epoch [10/10], Step [500/500], Loss: 1.3312\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e4d1c-3ebc-4718-9ea1-070dd57b6654",
   "metadata": {},
   "source": [
    "<h5><b>Step 5:</b> Evaluate the Model</h5>\n",
    "Evaluate the trained model on the test set to measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f92f6b50-ee62-41fe-8833-866c3daa12d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 55.40%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "'''\n",
    "torch.no_grad() => temporarily disable gradient calculations.\n",
    "It is helpful during inference (evaluation) because you don't need gradients for backpropagation when you are just evaluating the model.\n",
    "'''\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)   # finds the class with the highest score for each image. \n",
    "        total += labels.size(0)     #  count of the total number of samples processed so far. \n",
    "                                    #  label.size(0) gives the number of labels in the current batch, which is the same as the number of images in that batch.\n",
    "        correct += (predicted == labels).sum().item() # counts how many of the predicted labels match the true labels\n",
    "'''\n",
    "(predicted == label) creates a tensor of boolean values, where each element is True if the prediction matches the true label. \n",
    ".sum() counts the number of True values (i.e., correct predictions) in the batch. \n",
    ".item() converts the resulting tensor to a Python number, which is then added to correct_prediction.\n",
    "'''\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
